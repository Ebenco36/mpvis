{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e306bc3-bac3-4cb5-9820-0e7c7d7c7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def merge_pdfs(pdf_list, output_path):\n",
    "    merger = PyPDF2.PdfMerger()\n",
    "    \n",
    "    for pdf_file in pdf_list:\n",
    "        merger.append(pdf_file)\n",
    "    \n",
    "    merger.write(output_path)\n",
    "    merger.close()\n",
    "\n",
    "# Example usage:\n",
    "# pdf_list = [\n",
    "#     'Documents/PassportInformationPage.pdf', \n",
    "#     'Documents/VISAPage.pdf', \n",
    "#     'Documents/EmploymentContract.pdf',\n",
    "#     'Documents/Payments.pdf',\n",
    "#     'Documents/HealthInsuranceFrontPage.pdf', \n",
    "#     'Documents/HealthInsuranceBackPage.pdf',\n",
    "#     'Documents/Bachelors.pdf',\n",
    "#     'Documents/Masters.pdf', \n",
    "#     'Documents/Resume-Awotoro-Ebenezer-Oladimeji-3.pdf',\n",
    "#     'Documents/Confirmation.pdf',\n",
    "#     'Documents/HouseAgreement.pdf', \n",
    "#     'Documents/Wohnungsgeberbescheinigung.pdf'\n",
    "# ]\n",
    "pdf_list = [\n",
    "    'Documents/Form4.pdf', \n",
    "    'Documents/Form2.pdf', \n",
    "    'Documents/Form3.pdf',\n",
    "    'Documents/Form1.pdf'\n",
    "]\n",
    "output_path = 'Documents/RPFORM.pdf'\n",
    "merge_pdfs(pdf_list, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0769876f-22a1-424b-9c76-efdc0cae68d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd79a6f3-be97-4351-a4f6-d33eb982c8d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to locate or obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchDriverException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Replace the executable_path with the path to your chromedriver executable\u001b[39;00m\n\u001b[0;32m     29\u001b[0m service \u001b[38;5;241m=\u001b[39m ChromeService(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/chromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m driver:\n\u001b[0;32m     32\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Wait for dynamic content to load (adjust the timeout as needed)\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:51\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvendor_prefix \u001b[38;5;241m=\u001b[39m vendor_prefix\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[43mDriverFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:44\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate or obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to locate or obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.sciencedirect.com/science/article/pii/S2405852118300211\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "# Use requests to get the HTML content\n",
    "response = requests.get(url, headers={'User-Agent': user_agent})\n",
    "\n",
    "if response.status_code == 403:\n",
    "    # If you face a 403 error, use cloudscraper to bypass\n",
    "    import cloudscraper\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    response = scraper.get(url, headers={'User-Agent': user_agent})\n",
    "\n",
    "html_content = response.text\n",
    "\n",
    "# Use Selenium to interact with the page and get the fully loaded content\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode (without GUI)\n",
    "\n",
    "# Replace the executable_path with the path to your chromedriver executable\n",
    "service = ChromeService(\"path/to/chromedriver\")\n",
    "\n",
    "with webdriver.Chrome(service=service, options=chrome_options) as driver:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for dynamic content to load (adjust the timeout as needed)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, \"//your/dynamic/content/element\")))\n",
    "\n",
    "    # Get the page source after dynamic content loading\n",
    "    full_html_content = driver.page_source\n",
    "\n",
    "# Parse the fully loaded HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(full_html_content, 'html.parser')\n",
    "\n",
    "# Extract text from the parsed HTML\n",
    "complete_text = soup.get_text()\n",
    "\n",
    "# Save the text to a file\n",
    "output_file_path = \"output.txt\"\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(complete_text)\n",
    "\n",
    "print(f\"Complete text saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f53208d-1fb8-443f-9c27-0f1986e4e073",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxySchemeUnknown",
     "evalue": "Proxy URL had no scheme, should start with http:// or https://",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProxySchemeUnknown\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Replace the executable_path with the path to your chromedriver executable\u001b[39;00m\n\u001b[0;32m     12\u001b[0m service \u001b[38;5;241m=\u001b[39m ChromeService(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchromedriver.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m driver:\n\u001b[0;32m     15\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Wait for the dynamic content to load (adjust the timeout as needed)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# You might need to customize this part based on the specific behavior of the page\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# For example, waiting for a specific element to be present\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//your/xpath\")))\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the page source after dynamic content loading\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:57\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m---> 57\u001b[0m         command_executor\u001b[38;5;241m=\u001b[39m\u001b[43mChromiumRemoteConnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremote_server_addr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrowser_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvendor_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ignore_local_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     64\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m     65\u001b[0m     )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\chromium\\remote_connection.py:30\u001b[0m, in \u001b[0;36mChromiumRemoteConnection.__init__\u001b[1;34m(self, remote_server_addr, vendor_prefix, browser_name, keep_alive, ignore_proxy)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     24\u001b[0m     remote_server_addr: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     ignore_proxy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_server_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_proxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser_name \u001b[38;5;241m=\u001b[39m browser_name\n\u001b[0;32m     32\u001b[0m     commands \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_commands(vendor_prefix)\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:273\u001b[0m, in \u001b[0;36mRemoteConnection.__init__\u001b[1;34m(self, remote_server_addr, keep_alive, ignore_proxy)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_proxy_url() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_proxy \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_alive:\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_commands \u001b[38;5;241m=\u001b[39m remote_commands\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:244\u001b[0m, in \u001b[0;36mRemoteConnection._get_connection_manager\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_proxy_auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separate_http_proxy_auth()\n\u001b[0;32m    243\u001b[0m         pool_manager_init_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m urllib3\u001b[38;5;241m.\u001b[39mmake_headers(proxy_basic_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_proxy_auth)\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m urllib3\u001b[38;5;241m.\u001b[39mProxyManager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpool_manager_init_args)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib3\u001b[38;5;241m.\u001b[39mPoolManager(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpool_manager_init_args)\n",
      "File \u001b[1;32m~\\.conda\\envs\\jupyter_env\\lib\\site-packages\\urllib3\\poolmanager.py:563\u001b[0m, in \u001b[0;36mProxyManager.__init__\u001b[1;34m(self, proxy_url, num_pools, headers, proxy_headers, proxy_ssl_context, use_forwarding_for_https, proxy_assert_hostname, proxy_assert_fingerprint, **connection_pool_kw)\u001b[0m\n\u001b[0;32m    560\u001b[0m proxy \u001b[38;5;241m=\u001b[39m parse_url(str_proxy_url)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProxySchemeUnknown(proxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mport:\n\u001b[0;32m    566\u001b[0m     port \u001b[38;5;241m=\u001b[39m port_by_scheme\u001b[38;5;241m.\u001b[39mget(proxy\u001b[38;5;241m.\u001b[39mscheme, \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mProxySchemeUnknown\u001b[0m: Proxy URL had no scheme, should start with http:// or https://"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "url = \"https://www.sciencedirect.com/science/article/pii/S1078143917305288\"\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode (without GUI)\n",
    "\n",
    "# Replace the executable_path with the path to your chromedriver executable\n",
    "service = ChromeService(\"chromedriver.exe\")\n",
    "\n",
    "with webdriver.Chrome(service=service, options=chrome_options) as driver:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the dynamic content to load (adjust the timeout as needed)\n",
    "    # You might need to customize this part based on the specific behavior of the page\n",
    "    # For example, waiting for a specific element to be present\n",
    "    # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//your/xpath\")))\n",
    "\n",
    "    # Get the page source after dynamic content loading\n",
    "    full_page_content = driver.page_source\n",
    "\n",
    "# Now you have the complete HTML content of the page\n",
    "print(full_page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32571099-f157-4e49-a85f-930aa9ec53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "url = \"https://www.sciencedirect.com/science/article/pii/S2405852118300211\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "async def get_complete_text(url, user_agent):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Use async requests_html to render the JavaScript content\n",
    "    response = await session.get(url, headers={'User-Agent': user_agent})\n",
    "\n",
    "    # Wait for JavaScript to render (adjust the sleep time as needed)\n",
    "    await response.html.arender(sleep=5)\n",
    "\n",
    "    # Extract text from the rendered HTML\n",
    "    complete_text = response.html.text\n",
    "\n",
    "    # Save the text to a file\n",
    "    output_file_path = \"output.txt\"\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(complete_text)\n",
    "\n",
    "    print(f\"Complete text saved to {output_file_path}\")\n",
    "\n",
    "# Create an event loop and run the asynchronous function\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(get_complete_text(url, user_agent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd0ac26-544f-43db-a891-d1ac7833de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from requests-html) (2.31.0)\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Obtaining dependency information for fake-useragent from https://files.pythonhosted.org/packages/56/56/f72e9ca4f9cfb966f489c2b8ea04c67fa8d0cfbb62b1651cb9d6aef110a6/fake_useragent-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading fake_useragent-1.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting parse (from requests-html)\n",
      "  Obtaining dependency information for parse from https://files.pythonhosted.org/packages/9c/57/6c51ccd70de3ebcfb0bb5b0eea2ac0ab13c51ab55043a7243faef9eb58ef/parse-1.19.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached parse-1.19.1-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting bs4 (from requests-html)\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting w3lib (from requests-html)\n",
      "  Obtaining dependency information for w3lib from https://files.pythonhosted.org/packages/82/e2/dcf8573d7153194eb673347cea1f9bbdb2a8e61030740fb6f50e4234a00b/w3lib-2.1.2-py3-none-any.whl.metadata\n",
      "  Downloading w3lib-2.1.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 0.0/83.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.4/83.4 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2023.7.22)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (6.8.0)\n",
      "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.1)\n",
      "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Obtaining dependency information for urllib3<2.0.0,>=1.25.8 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading websockets-10.4-cp310-cp310-win_amd64.whl (101 kB)\n",
      "     ---------------------------------------- 0.0/101.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 101.4/101.4 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from bs4->requests-html) (4.12.2)\n",
      "Collecting lxml>=2.1 (from pyquery->requests-html)\n",
      "  Obtaining dependency information for lxml>=2.1 from https://files.pythonhosted.org/packages/50/ba/cb7bc9728a3be4e00dfd658fc76dc64fd9dbc3d5492ff44cda70574329c6/lxml-4.9.3-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading lxml-4.9.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from requests->requests-html) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from requests->requests-html) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (2.5)\n",
      "Downloading fake_useragent-1.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached parse-1.19.1-py2.py3-none-any.whl (18 kB)\n",
      "Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading lxml-4.9.3-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 16.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.9/3.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.7/3.8 MB 9.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/3.8 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.5/3.8 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.9/3.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.7/3.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.8/143.8 kB 8.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1265 sha256=f71e21b7c8e35cda5aba87623099285a58d8abc474229f793db3fcdddefc84db\n",
      "  Stored in directory: c:\\users\\awotoroe\\appdata\\local\\pip\\cache\\wheels\\25\\42\\45\\b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
      "Successfully built bs4\n",
      "Installing collected packages: pyee, parse, fake-useragent, appdirs, websockets, w3lib, urllib3, lxml, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.5\n",
      "    Uninstalling urllib3-2.0.5:\n",
      "      Successfully uninstalled urllib3-2.0.5\n",
      "Successfully installed appdirs-1.4.4 bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.3.0 lxml-4.9.3 parse-1.19.1 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 urllib3-1.26.18 w3lib-2.1.2 websockets-10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests-html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78257658-edaa-4699-88b8-abd227701d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Obtaining dependency information for scrapy from https://files.pythonhosted.org/packages/08/66/22ed9609df4b6d94a66512572a11b35943a6cb36dc268f88ebfbede60be1/Scrapy-2.11.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting Twisted<23.8.0,>=18.9.0 (from scrapy)\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "     ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/3.1 MB 960.0 kB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.5/3.1 MB 6.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.1 MB 7.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.6/3.1 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.8/3.1 MB 7.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.3/3.1 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 2.7/3.1 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.1/3.1 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.1/3.1 MB 8.0 MB/s eta 0:00:00\n",
      "Collecting cryptography>=36.0.0 (from scrapy)\n",
      "  Obtaining dependency information for cryptography>=36.0.0 from https://files.pythonhosted.org/packages/86/35/f03a42444866ef7f23134812a05012dcb509418214fb78ec848f28cd14b8/cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata\n",
      "  Downloading cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Obtaining dependency information for pyOpenSSL>=21.0.0 from https://files.pythonhosted.org/packages/db/de/007b832ad7a95e6a73745609bbe123c407aa2c46bb0b8f765c8718294e7f/pyOpenSSL-23.3.0-py3-none-any.whl.metadata\n",
      "  Downloading pyOpenSSL-23.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Obtaining dependency information for service-identity>=18.1.0 from https://files.pythonhosted.org/packages/0c/42/bf07f277b45da6e350df3314804aa2b5411e0938d3b78b4f17da2e1302c2/service_identity-23.1.0-py3-none-any.whl.metadata\n",
      "  Downloading service_identity-23.1.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from scrapy) (2.1.2)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Obtaining dependency information for zope.interface>=5.1.0 from https://files.pythonhosted.org/packages/97/7e/b790b4ab9605010816a91df26a715f163e228d60eb36c947c3118fb65190/zope.interface-6.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading zope.interface-6.1-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.8/42.8 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Obtaining dependency information for protego>=0.1.15 from https://files.pythonhosted.org/packages/bc/16/14fd1ecdece2e1d87279fc09fbd2d55bae5fa033783c3547af631c74d718/Protego-0.3.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading Protego-0.3.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from scrapy) (68.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from scrapy) (23.1)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Obtaining dependency information for tldextract from https://files.pythonhosted.org/packages/55/c8/43abd77a03143c7be1474df13d08118e1c37e7868e13d7bbc4a9a7d849eb/tldextract-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading tldextract-5.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from scrapy) (4.9.3)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.15.1)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
      "  Obtaining dependency information for constantly>=15.1 from https://files.pythonhosted.org/packages/b8/40/c199d095151addf69efdb4b9ca3a4f20f70e20508d6222bffb9b76f58573/constantly-23.10.4-py3-none-any.whl.metadata\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 0.0/74.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 74.6/74.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.8.0)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
      "  Obtaining dependency information for twisted-iocpsupport<2,>=1.0.2 from https://files.pythonhosted.org/packages/b1/2e/d99419d50e883efe30f8d17c0851307671058561643ba2d73f4f2873af97/twisted_iocpsupport-1.0.4-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading twisted_iocpsupport-1.0.4-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Obtaining dependency information for filelock>=3.0.8 from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n",
      "Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.4/286.4 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading cryptography-41.0.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.6/2.7 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.7 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.5/2.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.9/2.7 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.4/2.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
      "Downloading pyOpenSSL-23.3.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading zope.interface-6.1-cp310-cp310-win_amd64.whl (204 kB)\n",
      "   ---------------------------------------- 0.0/204.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 204.2/204.2 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading tldextract-5.1.0-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.7/97.7 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading twisted_iocpsupport-1.0.4-cp310-cp310-win_amd64.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.8/46.8 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: twisted-iocpsupport, PyDispatcher, incremental, zope.interface, queuelib, pyasn1, protego, jmespath, itemadapter, hyperlink, filelock, constantly, Automat, Twisted, requests-file, pyasn1-modules, parsel, cryptography, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-23.10.4 cryptography-41.0.5 filelock-3.13.1 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 pyOpenSSL-23.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-5.1.0 twisted-iocpsupport-1.0.4 zope.interface-6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e126a8c1-069c-4a91-aae7-2d3ce36512d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 00:57:36,079 INFO:Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2023-11-16 00:57:36,080 INFO:Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.5, Platform Windows-10-10.0.19044-SP0\n",
      "2023-11-16 00:57:36,085 INFO:Enabled addons:\n",
      "[]\n",
      "2023-11-16 00:57:36,091 WARNING:C:\\Users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-11-16 00:57:36,193 DEBUG:Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-11-16 00:57:36,239 INFO:Telnet Password: 1f0b295ade76a3da\n",
      "2023-11-16 00:57:36,531 INFO:Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-11-16 00:57:36,532 INFO:Overridden settings:\n",
      "{'LOG_ENABLED': False,\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
      "2023-11-16 00:57:36,722 INFO:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-11-16 00:57:36,736 INFO:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-11-16 00:57:36,737 INFO:Enabled item pipelines:\n",
      "[]\n",
      "2023-11-16 00:57:36,738 INFO:Spider opened\n",
      "2023-11-16 00:57:36,966 WARNING:C:\\Users\\awotoroe\\.conda\\envs\\jupyter_env\\lib\\site-packages\\pydispatch\\robustapply.py:44: RuntimeWarning: coroutine 'get_complete_text' was never awaited\n",
      "  for name in codeObject.co_varnames[startIndex:startIndex+len(arguments)]:\n",
      "\n",
      "2023-11-16 00:57:36,967 INFO:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-11-16 00:57:36,970 INFO:Telnet console listening on 127.0.0.1:6023\n",
      "2023-11-16 00:57:37,165 DEBUG:Starting new HTTPS connection (1): publicsuffix.org:443\n",
      "2023-11-16 00:57:37,582 DEBUG:https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 81653\n",
      "2023-11-16 00:57:37,695 DEBUG:Crawled (403) <GET https://www.sciencedirect.com/science/article/pii/S2405852118300211> (referer: None)\n",
      "2023-11-16 00:57:37,813 INFO:Ignoring response <403 https://www.sciencedirect.com/science/article/pii/S2405852118300211>: HTTP status code is not handled or not allowed\n",
      "2023-11-16 00:57:37,817 INFO:Closing spider (finished)\n",
      "2023-11-16 00:57:37,818 INFO:Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 343,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 5240,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 0.851288,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 11, 15, 23, 57, 37, 818934, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 6135,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 2,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 11, 15, 23, 57, 36, 967646, tzinfo=datetime.timezone.utc)}\n",
      "2023-11-16 00:57:37,819 INFO:Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Replace \"your_url_here\" with the actual URL\n",
    "        urls = [\"https://www.sciencedirect.com/science/article/pii/S2405852118300211\"]\n",
    "        for url in urls:\n",
    "            scraper = cloudscraper.create_scraper()\n",
    "            yield scraper.get(url, headers={'User-Agent': user_agent})\n",
    "            # yield scrapy.Request(url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response: HtmlResponse):\n",
    "        # Extract text from the response\n",
    "        complete_text = response.css('body ::text').getall()\n",
    "\n",
    "        # Concatenate the text into a single string\n",
    "        complete_text = ' '.join(complete_text)\n",
    "\n",
    "        # Save the text to a file\n",
    "        output_file_path = \"output.txt\"\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(complete_text)\n",
    "\n",
    "        self.log(f\"Complete text saved to {output_file_path}\")\n",
    "\n",
    "# Run the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'USER_AGENT': USER_AGENT,\n",
    "    'LOG_ENABLED': False,  # Disable logging to avoid cluttering the output\n",
    "})\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "941007f6-96a7-4c95-b85b-e7c429549b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 09:45:34,174 DEBUG:Starting new HTTPS connection (1): www.sciencedirect.com:443\n",
      "2023-11-16 09:45:35,252 DEBUG:https://www.sciencedirect.com:443 \"GET /science/article/pii/S2405852118300211 HTTP/1.1\" 200 None\n",
      "2023-11-16 09:45:35,262 Level 5:Code page ascii does not fit given bytes sequence at ALL. 'ascii' codec can't decode byte 0xe2 in position 974: ordinal not in range(128)\n",
      "2023-11-16 09:45:35,263 Level 5:Code page utf_8 is a multi byte encoding table and it appear that at least one character was encoded using n-bytes.\n",
      "2023-11-16 09:45:35,266 Level 5:utf_8 passed initial chaos probing. Mean measured chaos is 0.000000 %\n",
      "2023-11-16 09:45:35,267 Level 5:We detected language [('English', 0.7385), ('French', 0.6748), ('Italian', 0.647), ('Swedish', 0.6225), ('Norwegian', 0.6225), ('Danish', 0.6129), ('Dutch', 0.6118), ('Spanish', 0.6018), ('Indonesian', 0.5837), ('Portuguese', 0.5737), ('German', 0.5694), ('Slovene', 0.5466), ('Romanian', 0.5408), ('Czech', 0.5122), ('Estonian', 0.5102), ('Finnish', 0.4826), ('Polish', 0.4789), ('Slovak', 0.4765), ('Croatian', 0.4663), ('Turkish', 0.4485), ('Hungarian', 0.4485), ('Vietnamese', 0.4224), ('Lithuanian', 0.4208)] using utf_8\n",
      "2023-11-16 09:45:35,268 DEBUG:Encoding detection: utf_8 is most likely the one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burden of Human papillomavirus (HPV)-related disease and potential impact of HPV vaccines in the Republic of Korea - ScienceDirect\n",
      "JavaScript is disabled on your browser.\n",
      "      Please enable JavaScript to use all the features on this page.\n",
      "Skip to main content\n",
      "Skip to article\n",
      "ScienceDirect\n",
      "Journals & Books\n",
      "Search\n",
      "Register\n",
      "Sign in\n",
      "View\n",
      "PDF\n",
      "Download full issue\n",
      "Search ScienceDirect\n",
      "Papillomavirus Research\n",
      "Volume 7\n",
      ",\n",
      "June 2019\n",
      ", Pages 26-42\n",
      "Burden of Human papillomavirus (HPV)-related disease and potential impact of HPV vaccines in the Republic of Korea\n",
      "Author links open overlay panel\n",
      "Young-Tak\n",
      "Kim\n",
      "a\n",
      "1\n",
      ",\n",
      "Beatriz\n",
      "Serrano\n",
      "b\n",
      "f\n",
      "1\n",
      ",\n",
      "Jae-Kwan\n",
      "Lee\n",
      "c\n",
      ",\n",
      "Hyunju\n",
      "Lee\n",
      "d\n",
      ",\n",
      "Shin-Wha\n",
      "Lee\n",
      "a\n",
      ",\n",
      "Crystal\n",
      "Freeman\n",
      "b\n",
      ",\n",
      "Jin-Kyoung\n",
      "Oh\n",
      "e\n",
      ",\n",
      "Laia\n",
      "Alemany\n",
      "b\n",
      "g\n",
      ",\n",
      "Francesc-Xavier\n",
      "Bosch\n",
      "b\n",
      "f\n",
      ",\n",
      "Laia\n",
      "Bruni\n",
      "b\n",
      "f\n",
      "Show more\n",
      "Add to Mendeley\n",
      "Share\n",
      "Cite\n",
      "https://doi.org/10.1016/j.pvr.2018.12.002\n",
      "Get rights and content\n",
      "Under a Creative Commons\n",
      "license\n",
      "open access\n",
      "Highlights\n",
      "\n",
      "HPV-related disease burden (cancer and genital warts) in Korea is significant.\n",
      "\n",
      "HPV16 is the most frequent genotype, causing itself more than 60% of HPV-related cancers.\n",
      "\n",
      "HPV vaccine types 16/18/31/33/45/52/58/6/11 cause 92.0% of cervical cancers.\n",
      "\n",
      "HPV vaccines could significantly impact on the HPV-related disease burden.\n",
      "Abstract\n",
      "Background\n",
      "We aimed to review the burden and the potential impact of human papillomavirus (HPV) vaccines on HPV-related diseases in the Republic of Korea and to discuss cervical cancer prevention practices in this country.\n",
      "Methods\n",
      "Cancer burden statistics were retrieved from GLOBOCAN-2018 and Statistics Korea. HPV disease burden was assessed via systematic review. Vaccine types relative contribution (RC) was estimated using data from an international project using formalin-fixed paraffin-embedded specimens.\n",
      "Results\n",
      "Despite a downtrend in cervical cancer in recent years, Korean rates remain high. In contrast, oropharyngeal cancer incidence has gradually increased and other anogenital cancers remain rare.\n",
      "In Korea, HPV prevalence in general population is around 20%. In cervical cancer, RC of HPVs 16/18 (74.0%) increased to 92.0% when including HPVs 31/33/45/52/58. Limited information was available for other HPV-related cancer sites.\n",
      "Regarding prevention, since the inclusion of the HPV vaccine into the National Immunization Program, almost half (49%) of the target cohort in 2016 had received the first dose of vaccine. Further, percentage of women screened with pap has increased from 41.1%-2009 to 53.0%-2016.\n",
      "Conclusions\n",
      "HPV-related disease burden in Korea is significant. Results suggest that the combination of effective and high coverage HPV vaccination and screening programmes could substantially impact on HPV-related disease in Korea.\n",
      "Previous\n",
      "article\n",
      "in issue\n",
      "Next\n",
      "article\n",
      "in issue\n",
      "Keywords\n",
      "Human papillomavirus\n",
      "Cancer\n",
      "Burden\n",
      "Republic of Korea\n",
      "Papillomavirus vaccine\n",
      "Cervical cancer screening\n",
      "Recommended articles\n",
      "Cited by (0)\n",
      "1\n",
      "Equally contributed.\n",
      " 2019 The Authors. Published by Elsevier B.V.\n",
      "Recommended articles\n",
      "No articles found.\n",
      "Article Metrics\n",
      "View article metrics\n",
      "About ScienceDirect\n",
      "Remote access\n",
      "Shopping cart\n",
      "Advertise\n",
      "Contact and support\n",
      "Terms and conditions\n",
      "Privacy policy\n",
      "We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the\n",
      "use of cookies\n",
      ".\n",
      "All content on this site: Copyright \n",
      "2023\n",
      "Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.\n",
      "                                                Text\n",
      "0  Burden of Human papillomavirus (HPV)-related d...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import cloudscraper\n",
    "\n",
    "def scrape_website(url):\n",
    "    # Make an HTTP request to the website\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    response = requests.get(url, headers={'User-Agent': user_agent})\n",
    "    if(response.status_code == 403):\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        response = scraper.get(url, headers={'User-Agent': user_agent})\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get all the text from the HTML document\n",
    "        all_text = soup.get_text(strip=True, separator='\\n')\n",
    "        print(all_text)\n",
    "        # Create a pandas dataframe with a single column 'Text'\n",
    "        df = pd.DataFrame({'Text': [all_text]})\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# URL of the website to scrape\n",
    "website_url = 'https://www.sciencedirect.com/science/article/pii/S2405852118300211'\n",
    "\n",
    "# Call the scrape_website function and get the dataframe\n",
    "scraped_data = scrape_website(website_url)\n",
    "\n",
    "# Display the scraped data\n",
    "print(scraped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5271438-9d90-41b5-a3ad-38ecb690e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 17:11:01,708 DEBUG:Starting new HTTP connection (1): fw-bln.rki.local:8020\n",
      "2023-11-16 17:11:01,754 DEBUG:http://fw-bln.rki.local:8020 \"GET http://www.tandfonline.com/doi/full/10.1080/21645515.2019.1605818 HTTP/1.1\" 301 None\n",
      "2023-11-16 17:11:01,758 DEBUG:Starting new HTTPS connection (1): www.tandfonline.com:443\n",
      "2023-11-16 17:11:02,310 DEBUG:https://www.tandfonline.com:443 \"GET /doi/full/10.1080/21645515.2019.1605818 HTTP/1.1\" 403 None\n",
      "2023-11-16 17:11:02,463 DEBUG:Starting new HTTP connection (1): fw-bln.rki.local:8020\n",
      "2023-11-16 17:11:02,493 DEBUG:http://fw-bln.rki.local:8020 \"GET http://www.tandfonline.com/doi/full/10.1080/21645515.2019.1605818 HTTP/1.1\" 301 None\n",
      "2023-11-16 17:11:02,497 DEBUG:Starting new HTTPS connection (1): www.tandfonline.com:443\n",
      "2023-11-16 17:11:02,898 DEBUG:https://www.tandfonline.com:443 \"GET /doi/full/10.1080/21645515.2019.1605818 HTTP/1.1\" 403 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve content: 403 Client Error: Forbidden for url: https://www.tandfonline.com/doi/full/10.1080/21645515.2019.1605818\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_website_with_proxy(url, user_agent, proxy_url):\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    proxies = {'http': proxy_url, 'https': proxy_url}\n",
    "\n",
    "    try:\n",
    "        # Make an HTTP request to the website with the specified user agent and proxy\n",
    "        response = requests.get(url, headers=headers, proxies=proxies)\n",
    "        if(response.status_code == 403):\n",
    "            scraper = cloudscraper.create_scraper()\n",
    "            response = scraper.get(url, headers={'User-Agent': user_agent})\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get all the text from the HTML document\n",
    "        all_text = soup.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        # Create a pandas dataframe with a single column 'Text'\n",
    "        df = pd.DataFrame({'Text': [all_text]})\n",
    "        print(all_text)\n",
    "\n",
    "        return df\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve content: {e}\")\n",
    "        return None\n",
    "\n",
    "# URL of the website to scrape\n",
    "website_url = 'http://www.tandfonline.com/doi/full/10.1080/21645515.2019.1605818'\n",
    "# User agent string for a specific browser or device\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# Proxy URL with the correct scheme (http or https)\n",
    "proxy_url = 'http://fw-bln.rki.local:8020'\n",
    "\n",
    "# Call the scrape_website_with_proxy function with the specified user agent and proxy\n",
    "scraped_data = scrape_website_with_proxy(website_url, user_agent, proxy_url)\n",
    "\n",
    "# Display the scraped data\n",
    "print(scraped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bf538-6d68-4889-8e36-da5a0e38fe21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
