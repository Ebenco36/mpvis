{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c987319-8757-411f-a60e-f18231e701bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = {\n",
    "    \"standardScaler-KNN\", \n",
    "    'minmaxScaler-KNN', \n",
    "    'robustScaler-KNN', \n",
    "    'maxabsScaler-KNN',\n",
    "    # 'yeoJohnsonScaler-KNN',\n",
    "    'unitVectorScaler-KNN',\n",
    "    'mquantileTransformScaler-KNN',\n",
    "    \"standardScaler-softImputerRegressor\", \n",
    "    'minmaxScaler-softImputerRegressor', \n",
    "    'robustScaler-softImputerRegressor', \n",
    "    'maxabsScaler-softImputerRegressor',\n",
    "    # 'yeoJohnsonScaler-softImputerRegressor',\n",
    "    'unitVectorScaler-softImputerRegressor',\n",
    "    'mquantileTransformScaler-softImputerRegressor',\n",
    "    \"standardScaler-iterativeImputerRegressor\",\n",
    "    'minmaxScaler-iterativeImputerRegressor', \n",
    "    'robustScaler-iterativeImputerRegressor' , \n",
    "    'maxabsScaler-iterativeImputerRegressor',\n",
    "    # 'yeoJohnsonScaler-iterativeImputerRegressor',\n",
    "    'unitVectorScaler-iterativeImputerRegressor',\n",
    "    'mquantileTransformScaler-iterativeImputerRegressor',\n",
    "    \"standardScaler-simpleRegressor\", \n",
    "    'minmaxScaler-simpleRegressor', \n",
    "    'robustScaler-simpleRegressor', \n",
    "    'maxabsScaler-simpleRegressor',\n",
    "    # 'yeoJohnsonScaler-simpleRegressor',\n",
    "    'unitVectorScaler-simpleRegressor',\n",
    "    'mquantileTransformScaler-simpleRegressor'\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize dimensionality reduction techniques\n",
    "dim_reduction_techniques = {\n",
    "    'PCA',\n",
    "    'TruncatedSVD',\n",
    "    'FastICA',\n",
    "    'MDS',\n",
    "    't-SNE',\n",
    "    'UMAP',\n",
    "    'FactorAnalysis',\n",
    "    'Isomap',\n",
    "    'LocallyLinearEmbedding'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efcc849-3026-42d0-b1a2-aab792dc292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML for clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, OPTICS, SpectralClustering, MeanShift, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Function to prepend a string to each row\n",
    "def prepend_string(row, prefix):\n",
    "    return prefix + str(row)\n",
    "\n",
    "\n",
    "def kmeans_AL(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "    kmeans.fit(data)\n",
    "    labels = kmeans.labels_\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    # Get the cluster centers\n",
    "    centers = kmeans.cluster_centers_\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "\n",
    "def agglomerative_clustering_AL(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=3)\n",
    "    labels = hierarchical.fit_predict(data)\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "\n",
    "def DBSCAN_AL(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "\n",
    "def mean_shift(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    meanshift = MeanShift()\n",
    "    labels = meanshift.fit_predict(data)\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "    \n",
    "def gaussian_AL(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    gmm = GaussianMixture(n_components=3)\n",
    "    labels = gmm.fit_predict(data)\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "\n",
    "def spectral_AL(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    spectral = SpectralClustering(n_clusters=3)\n",
    "    labels = spectral.fit_predict(data)\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "\n",
    "def optics_AL(df):\n",
    "    # Create a copy of the original DataFrame\n",
    "    data = df.copy()\n",
    "    optics = OPTICS(min_samples=5, xi=0.05)\n",
    "    labels = optics.fit_predict(data)\n",
    "    data['labels'] = labels\n",
    "    data['labels_tagged'] = data['labels'].apply(prepend_string, args=(\"Group \",))\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    return data, labels, np.unique(labels), num_clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dir(directory_path):\n",
    "    import os\n",
    "    # Check if the directory exists, and if not, create it\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Directory '{directory_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d12c90-29f7-4f7d-9c78-16ef08fbc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import random\n",
    "import altair as alt\n",
    "\n",
    "# Initialize ML techniques\n",
    "ML_techniques = {\n",
    "    'kmeans_AL': 'kmeans_AL',\n",
    "    'DBSCAN_AL': 'DBSCAN_AL',\n",
    "    'mean_shift': 'mean_shift',\n",
    "    'gaussian_AL': 'gaussian_AL',\n",
    "    'optics_AL': 'optics_AL',\n",
    "    'agglomerative_clustering_AL': 'agglomerative_clustering_AL',\n",
    "}\n",
    "\n",
    "charts = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    for dm in dim_reduction_techniques:\n",
    "        for technique_name, technique in ML_techniques.items():\n",
    "            create_dir(\"cluster_charts/\" + dataset+\"_\"+technique_name+\"/charts/\")\n",
    "            create_dir(\"cluster_datasets/\" + dataset+\"_\"+technique_name+\"/datasets/\")\n",
    "            clustered_data, labels, comp, compp = eval(technique)(datasets[dataset])\n",
    "            print(compp)\n",
    "            # Define specific colors for each label\n",
    "            color_tag = ['#93C4F6', '#005EB8', '#D9DE84', '#636B05']\n",
    "            if(compp > 1):\n",
    "                color_scale = alt.Scale(domain=np.unique(clustereds_data[\"labels_tagged\"]), range=color_tag)\n",
    "                clustered_data.to_csv(\"cluster_datasets/\" + dataset+\"_\"+technique_name+\"/datasets/\"+technique_name + '_clustering.csv')\n",
    "                # Plot the data points and cluster centers\n",
    "                clustered_data\n",
    "                \n",
    "                # Evaluate clustering using different metrics\n",
    "                silhouette = silhouette_score(clustered_data[[\"PCA1\", \"PCA2\", \"labels\"]], labels)\n",
    "                db_index = davies_bouldin_score(clustered_data[[\"PCA1\", \"PCA2\", \"labels\"]], labels)\n",
    "                ch_score = calinski_harabasz_score(clustered_data[[\"PCA1\", \"PCA2\", \"labels\"]], labels)\n",
    "                \n",
    "                chart = alt.Chart(clustered_data).mark_point().encode(\n",
    "                    x='PCA1:Q',\n",
    "                    y='PCA2:Q',\n",
    "                    color=alt.Color('labels_tagged:N', scale=color_scale, legend=alt.Legend(title=\"Clusters\")),  # Use the defined color scale\n",
    "                    tooltip=['PCA1', 'PCA2', 'labels_tagged'],  # Add tooltip information\n",
    "                    # text=\"{'silhouette':\" + str(silhouette) + \", 'davies_bouldin_score':\" + str(db_index) + \", 'calinski_harabasz_score':\" + str(ch_score) + \"}\"\n",
    "                ).properties(title=f'{technique_name} on {dataset.replace(\"_\", \" \")}')\n",
    "                # charts.append(chart)\n",
    "                # combine_chart = alt.hconcat(*charts)\n",
    "                # combine_chart\n",
    "                \n",
    "                chart.save(\"cluster_charts/\" + dataset+\"_\"+technique_name+\"/charts/\"+dataset + 'HIGH.png', engine=\"vl-convert\", ppi=200, format='png')\n",
    "    \n",
    "        \n",
    "                # Print the evaluation results\n",
    "                print(f\"Silhouette Score: {silhouette}\")\n",
    "                print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "                print(f\"Calinski-Harabasz Index: {ch_score}\")\n",
    "                # print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "                # print(f\"Normalized Mutual Information: {nmi_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee9f89-716d-4d14-a9e9-6ada68461258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
